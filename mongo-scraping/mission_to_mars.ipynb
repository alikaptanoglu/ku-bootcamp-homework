{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "import tweepy\n",
    "import os\n",
    "import re\n",
    "import pymongo\n",
    "\n",
    " \n",
    "# chrome driver\n",
    "executable_path = {'executable_path': 'chromedriver.exe'}\n",
    "browser = Browser(\"chrome\", **executable_path, headless=False)\n",
    "\n",
    "# API keys\n",
    "api_dir = os.path.dirname(os.path.dirname(os.path.realpath('keys')))\n",
    "file_name = os.path.join(api_dir + \"//keys\", \"api_keys.json\")\n",
    "data = json.load(open(file_name))\n",
    "\n",
    "consumer_key = data['twitter_consumer_key']\n",
    "consumer_secret = data['twitter_consumer_secret']\n",
    "access_token = data['twitter_access_token']\n",
    "access_token_secret = data['twitter_access_token_secret']\n",
    "\n",
    "# tweepy setup\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth, parser=tweepy.parsers.JSONParser())\n",
    "\n",
    "# use mongodb\n",
    "conn = 'mongodb://localhost:27017'\n",
    "client = pymongo.MongoClient(conn)\n",
    "db_planetdetails = client.planetdetails\n",
    "db_planetfacts = client.planetfacts\n",
    "db_planetimages = client.planetimages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scrape_latest_news():\n",
    "    url = 'https://mars.nasa.gov/news/?page=0&per_page=40&order=publish_date+desc%2Ccreated_at+desc&search=&category=19%2C165%2C184%2C204&blank_scope=Latest'\n",
    "    browser.visit(url)\n",
    "    time.sleep(1)\n",
    "    html = browser.html\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    result = soup.find('li', class_='slide')\n",
    "    \n",
    "    title = 'not found'\n",
    "    para = 'not found'\n",
    "    try:\n",
    "        title = result.find('div', class_='content_title').text\n",
    "        para = result.find('div', class_='article_teaser_body').text\n",
    "        # only do it once and return\n",
    "        return {'title': title, 'para': para}\n",
    "    except Exception as e:\n",
    "        print('error ', e)\n",
    "        return {'title': title, 'para': para}\n",
    "\n",
    "\n",
    "def scrape_feature_image():\n",
    "    url = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'\n",
    "    browser.visit(url)\n",
    "    time.sleep(1)\n",
    "    html = browser.html\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    result = soup.find('article', class_='carousel_item')\n",
    "    try:\n",
    "        style = result['style']\n",
    "        img = style.replace('background-image: url(\\'', '')\n",
    "        img = img.replace(\"');\", '')\n",
    "        return img\n",
    "\n",
    "    except Exception as e:\n",
    "        print('error ', e)\n",
    "        return 'not found'\n",
    "    \n",
    "    \n",
    "        \n",
    "def scrape_latest_weather_tweet():\n",
    "    mars_status = api.get_user('@MarsWxReport')\n",
    "    return mars_status['status']['text']\n",
    "    \n",
    "    \n",
    "\n",
    "def scrape_mars_facts():\n",
    "    url = 'https://space-facts.com/mars/'\n",
    "    browser.visit(url)\n",
    "    time.sleep(1)\n",
    "    html = browser.html\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    facts_found = soup.find_all('tr', {'class' : re.compile('row*')})\n",
    "    \n",
    "    labels = []\n",
    "    facts = []\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    try:\n",
    "        for fact in facts_found:\n",
    "            details = fact.find_all('td', {'class' : re.compile('column*')})\n",
    "            labels.append(details[0].text)\n",
    "            facts.append(details[1].text)\n",
    "            \n",
    "        df['label'] = labels\n",
    "        df['fact'] = facts\n",
    "        \n",
    "    except Exception as e:\n",
    "        print('error ', e)\n",
    "        \n",
    "    return df\n",
    "        \n",
    "        \n",
    "\n",
    "def scrape_hemisphere_images():\n",
    "    url = 'https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars'\n",
    "    browser.visit(url)\n",
    "    time.sleep(1)\n",
    "    html = browser.html\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    astro_url = 'https://astrogeology.usgs.gov'\n",
    "    \n",
    "    results = soup.find_all('div', class_='description')\n",
    "\n",
    "    images = []\n",
    "    \n",
    "    try:\n",
    "        for result in results:\n",
    "            # look into each hemisphere\n",
    "            hemis = result.find('a', class_=\"itemLink\")\n",
    "            \n",
    "            # click the url\n",
    "            browser.visit(astro_url + hemis['href'])\n",
    "            html = browser.html\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            href = soup.find('a', {'target' : '_blank'})\n",
    "           \n",
    "            # build the data dictionary\n",
    "            each_image = {'title': text, 'image_url': href['href']}\n",
    "            images.append(each_image)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print('error ', e)\n",
    "\n",
    "    return images\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scrape_all():\n",
    "    # remove old mars data\n",
    "    planetdetails = db_planetdetails.listings\n",
    "    planetdetails.drop()\n",
    "    planetdetails = db_planetdetails.listings\n",
    "\n",
    "    planetfacts = db_planetfacts.listings\n",
    "    planetfacts.drop()\n",
    "    planetfacts = db_planetfacts.listings\n",
    "\n",
    "    planetimages = db_planetimages.listings\n",
    "    planetimages.drop()\n",
    "    planetimages = db_planetimages.listings\n",
    "\n",
    "    planetdetail = {}\n",
    "    planetfact = {}\n",
    "    planetimage = {}\n",
    "\n",
    "    #1. latest news title and paragraph\n",
    "    title_para = scrape_latest_news()\n",
    "    planetdetail['latest_news_title'] = title_para['title']\n",
    "    planetdetail['latest_news_para'] = title_para['para']\n",
    "\n",
    "    #2. feature image\n",
    "    jpl_url = 'https://www.jpl.nasa.gov'\n",
    "    feature_image_jpg = scrape_feature_image()\n",
    "    feature_image_url = jpl_url + feature_image_jpg\n",
    "    planetdetail['feature_image_url'] = feature_image_url\n",
    "    \n",
    "    #3. latest weather tweet\n",
    "    mars_weather = scrape_latest_weather_tweet()\n",
    "    planetdetail['mars_weather'] = mars_weather\n",
    "\n",
    "    # add new row to the planetdetails table\n",
    "    planetdetails.insert_one(planetdetail)\n",
    "\n",
    "    #4. MARS facts\n",
    "    mars_facts = scrape_mars_facts()\n",
    "    for i, row in mars_facts.iterrows():\n",
    "        planetfact = {'label': row['label'], 'fact': row['fact']}\n",
    "        planetfacts.insert_one(planetfact)\n",
    "\n",
    "    #5. high resolution images of each hemisphere\n",
    "    images = scrape_hemisphere_images()\n",
    "    for image in images:\n",
    "        planetimage = {'label': image['title'], 'image_url': image['image_url']}\n",
    "        planetimages.insert_one(planetimage)\n",
    "        \n",
    "    # close the current browser tab when the scraping is done\n",
    "    browser.driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scrape_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda]",
   "language": "python",
   "name": "conda-env-Anaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
